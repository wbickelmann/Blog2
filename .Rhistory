sahdd.kn <- as.data.frame(lapply(sahdd[1:9], normalize))
sahdd.kn$chd<-sahdd$chd +1
x<-sahdd.kn[-10]
y<-sahdd.kn$chd
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.kn <- as.data.frame(lapply(sahdd[1:9], normalize))
sahdd.kn$chd<-sahdd$chd +1
x<-sahdd.kn[-10]
y<-sahdd.kn$chd
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 19, regression = FALSE, Levels = unique(y), method = 'euclidean',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 19, regression = FALSE, Levels = unique(y), method = 'minkowski',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 19, regression = FALSE, Levels = unique(y), method = 'mahalanobis',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
sahdd_test_pred
sahdd_test_pred
knitr::opts_chunk$set(echo = TRUE)
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
install.packages("KernelKnn")
install.packages("ks")
install.packages("class")
library(class)
library(KernelKnn)
library(ks)
library(caret)
install.packages("KernelKnn")
install.packages("ks")
install.packages("class")
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
install.packages("class")
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.n <- as.data.frame(lapply(sahdd[1:10], normalize))
sahdd_train.kn<-sahdd.n[-10]
sahdd_train_labels.kn<-sahdd$chd
sahdd_test_pred <-knn.cv(sahdd_train.kn, sahdd_train_labels.kn, k = 1, prob = TRUE)
confusionMatrix(sahdd_test_pred,sahdd_train_labels.kn)
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.n <- as.data.frame(lapply(sahdd[1:10], normalize))
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.n <- as.data.frame(lapply(sahdd[1:10], normalize))
sahdd_train.kn<-sahdd.n[-10]
sahdd_train_labels.kn<-sahdd$chd
sahdd_test_pred <-knn.cv(sahdd_train.kn, sahdd_train_labels.kn, k = 5, prob = TRUE)
confusionMatrix(sahdd_test_pred,sahdd_train_labels.kn)
sahdd_test_pred <- knn.cv(sahdd_train.kn, sahdd_train_labels.kn, k = 11, prob = TRUE)
confusionMatrix(sahdd_test_pred,sahdd_train_labels.kn)
sahdd_test_pred <- knn.cv(sahdd_train.kn, sahdd_train_labels.kn, k = 19, prob = TRUE)
confusionMatrix(sahdd_test_pred,sahdd_train_labels.kn)
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.k<-sahdd
sahdd.k$chd<-sahdd.k$chd +1
x<-sahdd.k[-10]
y<-sahdd.k$chd
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 5, regression = FALSE, Levels = unique(y), method = 'euclidean',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.k<-sahdd
sahdd.k$chd<-sahdd.k$chd +1
x<-sahdd.k[-10]
y<-sahdd.k$chd
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.kn <- as.data.frame(lapply(sahdd[1:9], normalize))
sahdd.kn$chd<-sahdd$chd +1
x<-sahdd.kn[-10]
y<-sahdd.kn$chd
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 19, regression = FALSE, Levels = unique(y), method = 'euclidean',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
sahdd<-sahdd[-1]
sahdd$famhist<-as.numeric(sahdd$famhist == "Present")
sahdd.kn <- as.data.frame(lapply(sahdd[1:9], normalize))
sahdd.kn$chd<-sahdd$chd +1
x<-sahdd.kn[-10]
y<-sahdd.kn$chd
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 5, regression = FALSE, Levels = unique(y), method = 'euclidean',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 5, regression = FALSE, Levels = unique(y), method = 'minkowski',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
outKNN <- KernelKnn(x, TEST_data = NULL, as.numeric(y), k = 5, regression = FALSE, Levels = unique(y), method = 'mahalanobis',weights_function = 'exponential')
Classinto <- c(0,0,0)
for (i in seq(length(y)))
{ Classinto[i] <- match(1,match(outKNN[i,], max(outKNN[i,])))
}
confusionMatrix(Classinto,y)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
library(dplyr)
library(ggplot)
library(ggiraph)
Batting%>%
group_by(playerID)%>%
summarize(Career_HR=sum(HR),Career_SO=sum(SO))%>%
filter(Career_HR>=400)->df
HR_vs_SO<-inner_join(df,Master,by=c("playerID"))%>%
select(nameFirst,nameLast,Career_HR,Career_SO)
#----------------------------------------------------------------------------------
g<-ggplot()+
geom_point(data=HR_vs_SO, aes(x=Career_SO,y=Career_HR, tooltip=nameLast)) +
ggtitle("Career Homeruns vs. Strikeouts for Great Hitters") +
xlab("Strikeouts") +
ylab("Homeruns")
ggiraph(code=print(g),hover_css="fill:white;stroke:black")
## Packages for Social Network Analysis
library(igraph)
library(graphTweets)
### Scraping Twitter Data in R and Text Analysis
library(twitteR)
library(ROAuth)
library(httr)
library(plyr)
library(tm)
library(topicmodels)
### Sentiment Analysis with Twitter Data
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
Accuracies <- c(0.00)
for (i in seq(500))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
pda4 <- train(PopSex ~ ., data = training, method = "multinom",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(pda4, list(.decay = 3))
pda4_pred <- predict(pda4,newdata = testing)
Accuracies[i] <- confusionMatrix(pda4_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(500))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
pda4 <- train(PopSex ~ ., data = training, method = "multinom",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(pda4, list(.decay = 3))
pda4_pred <- predict(pda4,newdata = testing)
Accuracies[i] <- confusionMatrix(pda4_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
library(caret)
library(MASS)
Accuracies <- c(0.00)
for (i in seq(500))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
pda4 <- train(PopSex ~ ., data = training, method = "multinom",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(pda4, list(.decay = 3))
pda4_pred <- predict(pda4,newdata = testing)
Accuracies[i] <- confusionMatrix(pda4_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
length(Accuracies)
summary(Accuracies)
confusionMatrix(pda4_pred,testing$PopSex)
library(caret)
library(MASS)
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
# many columns are Nas
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
Accuracies <- c(0.00)
for (i in seq(100)) # takes a while
{
inTrain <- createDataPartition(H4A$PopSex, p = .75, list = FALSE) # compact now
training <- H4A[inTrain,]
test <- H4A[-inTrain,]
HHP <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "rpart",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.control(minsplit = 20, cp = 0.01)) # bootstrap
#trControl = trainControl(method = "cv"))
mult_pred <- predict(HHP,newdata = test)
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "lda",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
trControl = trainControl(method = "cv"))
mult_pred <- predict(mult,newdata = testing)
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100)) # takes a while
{
inTrain <- createDataPartition(H4A$PopSex, p = .75, list = FALSE) # compact now
training <- H4A[inTrain,]
test <- H4A[-inTrain,]
HHP <- train(PopSex ~., data = training,
method = 'lda',
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
trControl = trainControl(method = "cv"))
mult_pred <- predict(H4A,newdata = test)
Accuracies[i] <- confusionMatrix(multi_pred,test$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ., data = training,
method = "lda",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
trControl = trainControl(method = "cv"))
mult_pred <- predict(mult,newdata = testing)
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ., data = training,
method = "rpart",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
trControl = trainControl(method = "cv"))
mult_pred <- predict(mult,newdata = testing)
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "lda",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
trControl = rpart.Control(minisplit = 20, cp = .01))
mult_pred <- predict(mult,newdata = testing)
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "lda",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
trControl = rpart.Control(minisplit = 20, cp = .01))
mult_pred <- predict(mult,newdata = H4A[-inTrain,])
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "rpart",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
trControl = rpart.Control(minisplit = 20, cp = .01))
mult_pred <- predict(mult,newdata = H4A[-inTrain,])
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "rpart",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.Control(minisplit = 20, cp = .01))
mult_pred <- predict(mult,newdata = H4A[-inTrain,])
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "rpart",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.Control(minsplit = 20, cp = .01))
mult_pred <- predict(mult,newdata = H4A[-inTrain,])
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "rpart",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.Control(minsplit = 20, cp = .01))
mult_pred <- predict(mult,newdata = H4A[-inTrain,])
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
Accuracies <- c(0.00)
for (i in seq(100))
{
inTrain <- createDataPartition(H4A$PopSex, p = .75, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
mult <- train(PopSex ~ ZYB + NOL + FOL + STB + GLS + BBH + AUB + DKB + EKB + ZOR, data = training,
method = "rpart",
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.control(minsplit = 20, cp = .01))
mult_pred <- predict(mult,newdata = H4A[-inTrain,])
Accuracies[i] <- confusionMatrix(mult_pred,testing$PopSex)$overall["Accuracy"]}
summary(Accuracies)
plot(density(Accuracies))
setwd("~/Blog2")
blogdown::serve_site()
## Packages for Social Network Analysis
library(igraph)
library(graphTweets)
### Scraping Twitter Data in R and Text Analysis
library(twitteR)
library(ROAuth)
library(httr)
library(plyr)
library(tm)
library(topicmodels)
library(rpart.plot)
library(party)
library(rpart)
### Sentiment Analysis with Twitter Data
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
## Packages for Social Network Analysis
library(igraph)
library(graphTweets)
### Scraping Twitter Data in R and Text Analysis
library(twitteR)
library(ROAuth)
library(httr)
library(plyr)
library(tm)
library(topicmodels)
library(rpart.plot)
library(party)
library(rpart)
### Sentiment Analysis with Twitter Data
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
install.packages("party")
library(party)
# Set API Keys
api_key <- "Y1b2lIYNjFM75vYChQS0awU7q"
api_secret <- "zyvOclhto5HBiP9T9zf1Q3gCVfrEwtj7t7WcfxNSv69a3DvxNp"
access_token <- "902614141160054784-4DsATWhXlokkkwwe1Wj4p11zqCogvld"
access_token_secret <- "MuPepIO1visl3rLX50Mw6RRC5aPIPcRuLzHYFKy2Lxp7p"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
# Set API Keys
api_key <- "Y1b2lIYNjFM75vYChQS0awU7q"
api_secret <- "zyvOclhto5HBiP9T9zf1Q3gCVfrEwtj7t7WcfxNSv69a3DvxNp"
access_token <- "902614141160054784-4DsATWhXlokkkwwe1Wj4p11zqCogvld"
access_token_secret <- "MuPepIO1visl3rLX50Mw6RRC5aPIPcRuLzHYFKy2Lxp7p"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets <- searchTwitter("anthem", n=1000, lang="en")
tw_df <- twListToDF(tweets)
# Labeling our network
classified = cbind(tw_df, rep("anthem", nrow(tw_df)))
colnames(classified)[ncol(classified)]="classifier"
tw_df = classified
tweets2 <- searchTwitter("takeaknee", n=1000, lang="en")
tw_df_2 <- twListToDF(tweets2)
# Labeling our network
classified = cbind(tw_df_2, rep("takeaknee", nrow(tw_df_2)))
colnames(classified)[ncol(classified)]="classifier"
tw_df_2 = classified
#combining Dataframes
total_en = rbind.fill(tw_df_2, tw_df)
str(total_en)
#### Text Preprocessing
sk = total_en$text
TextPreprocessing = lapply(sk, function(x) {
x = gsub('http\\S+\\s*', '', x) ## Remove URLs
x = gsub('\\b+RT', '', x) ## Remove RT
x = gsub('#\\S+', '', x) ## Remove Hashtags
x = gsub('@\\S+', '', x) ## Remove Mentions
x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
x = gsub("\\d", '', x) ## Remove Controls and special characters
x = gsub('[[:punct:]]', '', x) ## Remove Punctuations
x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
})
# or as a vector
bd_list = as.vector(TextPreprocessing)
mycorpus <- Corpus(VectorSource(bd_list))
mycorpus = tm_map(mycorpus, content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')))
### Transformer all characters to lower case
mycorpus = tm_map(mycorpus, content_transformer(tolower))
### Remove all Punctuation
mycorpus = tm_map(mycorpus, removePunctuation)
### Remove all Numbers
mycorpus = tm_map(mycorpus, removeNumbers)
### Remove Stopwords
mycorpus = tm_map(mycorpus, removeWords, stopwords('english'))
#### transform to Document Term Matrix
skip.dtm = DocumentTermMatrix(mycorpus)
#### Supervised Machine Learning
### From matrix to dataframe
dtm.m = as.matrix(skip.dtm)
dtm_total = as.data.frame(dtm.m, stringsAsFactors=FALSE)
## Selecting just the classifier variable
classifier = total_en$classifier
##combining the classifier variable with our dataframe
total_final = cbind(classifier, dtm_total)
### Data preparation for Supervised Machine Learning
### Transforming all NAs to O
total_final[is.na(total_final)]=0
### Checking our dataframes structure
str(total_final)
## Create Training and Test Data Set
trainmodel = sample(nrow(total_final), ceiling(nrow(total_final)*0.7))
testmodel = (1:nrow(total_final)) [-trainmodel]
traindata = total_final[trainmodel, ]
testdata = total_final[testmodel, ]
traindata = data.frame(traindata)
testdata = data.frame(testdata)
## Running a Supervised Machine Learning Analysis (Classification Tree)
mintest = rpart(classifier~., data = traindata, method = "class")
pred = predict(mintest, newdata=testdata, type = "class")
mc = table(pred, testdata$classifier)
err = 1.0 - (mc[1,1]+mc[2,2])/sum(mc)
mc
err
rpart.plot(mintest, tweak = 1.2, main="TakeAKnee vs. Anthem")
View(traindata)
